---
title: "Project 2: STAT302package Tutorial"
author: "Wayne Wang"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{STAT302package Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

STAT302package contains four functions that are commoned used to solve statistical problems:  

* my_t.test: Performs a one sample t-test in R.
* my_lm: Fits a linear model in R.
* my_knn_cv: Performs a k-nearest neighbors cross-validation.
* my_rf_cv: Tests how good the random forest model fits in the penguins data set.  

This document will demonstrate the use of these functions with `penguins` and `gapminder` data, 
which are stored within this package as `my_penguins` and `gapminder`.  

First, install the `STAT302package` from Github with following instruction:
```{r, eval = FALSE}
# install.packages("devtools")
devtools::install_github("WayneWang86/STAT302package")
```

```{r setup}
library(STAT302package)
library(ggplot2)
library(tidyverse)
library(class)
library(randomForest)
library(knitr)
library(kableExtra)
```

## my_t.test Tutorial  

### Demonstrate a test of the hypothesis: mean = 60 (null); mean != 60 (alternative)
In the hypothesis test, the alternative hypothesis is that mean != 60, therefore  
we use "two.sided" as the value for `alternative`

```{r}
# testing when alternative equals two.sided
p_val_1 <- (my_t.test(my_gapminder$lifeExp, alternative = "two.sided", mu = 60))$p_val
p_val_1
```
Bsaed on the result from `my_t.test` function, The p-value is `r p_val_1`. Since the  
p-value cut-off is 0.05 and the p-value we got here is larger than the cut-off value, 
we fail to reject the null hypothesis that the mean life expectancy is 60.

### Demonstrate a test of the hypothesis: mu = 60 (null); mu < 60 (alternative)  
In the hypothesis test, the alternative hypothesis is that mean < 60, therefore  
we use "less" as the value for `alternative`
```{r}
# testing when alternative equals less
p_val_2 <- (my_t.test(my_gapminder$lifeExp, alternative = "less", mu = 60))$p_val
p_val_2
```
Bsaed on the result from `my_t.test` function, The p-value is `r p_val_2`. Since the  
p-value cut-off is 0.05 and the p-value we got here is smaller than the cut-off value, 
we have sufficient evidence to reject the null hypothesis and conclude that the true 
mean life expectancy is less than 60.

### Demonstrate a test of the hypothesis: mu = 60 (null); mu > 60 (alternative)  
In the hypothesis test, the alternative hypothesis is that mean > 60, therefore  
we use "greater" as the value for `alternative`
```{r}
# testing when alternative equals greater
p_val_3 = (my_t.test(my_gapminder$lifeExp, alternative = "greater", mu = 60))$p_val
p_val_3
```
Bsaed on the result from `my_t.test` function, The p-value is `r p_val_3`. Since the  
p-value cut-off is 0.05 and the p-value we got here is much larger than the cut-off value, 
we fail to reject the null hypothesis.  


## A tutorial for my_lm  

The task here is to use `my_lm` function to demonstrate a regression using lifeExp as your  
response variable and gdpPercap and continent as explanatory variables.

```{r}
lifeExp_lm <- my_lm(lifeExp ~ gdpPercap+continent, my_gapminder)
lifeExp_lm
```
Here we will take a close look at the `gdpPercap` coefficient in this linear model. The `gdpPercap`
coefficient means that with other covariates stay unchanged, when the `gdpPercap` increase by one
dollar, the life expectancy would increase by around 0.0004 year. This indicates that there is a 
positive correlation between `gdpPercap` and `lifeExp`.  

To perform a hypothesis test associated with the `gdpPercap` coefficient, we can set the null 
hypothesis as that the `gdpPercap` coefficient is 0 (meaning it has no effects on the value 
of `lifeExp`), and the alternative hypothesis would be that the `gdpPercap` coefficient is not 0.  
As we can see from the table, the related p-value for `gdpPercap` is extremely small, which 
obviously is smaller than the cut-off value of 0.05, therefore, we have sufficient evidence to 
reject the null hypothesis that the `gdpPercep` coefficient is 0.  

We will then use `ggplot2` library to plot out the Actual and fitted values
```{r}
# get coefficients from all covariates
coef <- lifeExp_lm$Estimate

# calculate the fitted values
my_matrix <- model.matrix(lifeExp ~ gdpPercap + continent, 
                          data = my_gapminder)
y_hat <- my_matrix %*% as.matrix(coef)

# create data frame with actual data, fitted data and the continent variable
my_data <- data.frame("actual" = my_gapminder$lifeExp,
                      "fitted" = y_hat,
                      "continent" = my_gapminder$continent)

# create plot
my_plot <- ggplot(my_data, aes(x = fitted, y = actual, color = continent)) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0, col = "red") + 
  labs(title = "Actual v.s. Fitted") + 
  theme_bw(base_size = 10) + 
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 12))
my_plot
```

We can see that the plots for Africa, America and Asia are almost vertical. It indicates that
for these three continents, our linear model would predict the life expectancy at a specific range
for a specific continent. For example, in Africa, the actual life expectancy is ranging from 
30 to 80 while the fitted life expectancy is concertrate around 50 years old.  
In addition, our model predict that Africa countries would have the shortest life expectancy while
Europe and Oceania would have the longest life expectancy.  
From the plots we can see very clearly that this linear model doesn't fit very well, especially
for Africa, Asia and Americas, as those data points are almost vertical lines rather than being
around the best fit line. Therefore, we would conclude that this linear model doesn't fit the 
data too well and has poor quality to make prediction with.  


## A tutorial for `my_knn_cv` function by using `my_peguins`  

This demonstration we will be using the `my_knn_cv` function with `my_peguins` data to predict  
output class `species` using covariates `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`  
and `body_mass_g` Here will will use 5-fold cross validation (k_cv = 5) and predict the output
class `species` with with 1-nearest neighbor through 10-nearest neighbors.

```{r}
penguins <- na.omit(my_penguins[, 1:6])
# create a matrix to store outputs
matrix <- matrix(NA, nrow = 10, ncol = 3)

# Performs k-nearest neighbors cross-validation with k from 1 to 10
for (i in 1 : 10) {
  # store the value of k_nn in matrix
  matrix[i, 1] <- i
  # get output list from my function
  output <- my_knn_cv(penguins[,3:6], penguins$species, k_cv = 5, k_nn = i)
  prediction <- output$class
  # store training misclassification rate
  matrix[i, 2] <- mean(as.numeric(prediction != penguins$species) ** 2) / nrow(penguins)
  # store CV misclassification rate
  matrix[i, 3] <- output$cv_err
}
# # create data frame to hold columns from my matrix
err_compare <- data.frame("k_nn" = matrix[, 1],
                      "training_misclassification_rate" = matrix[, 2],
                      "CV_misclassification_rate" = matrix[, 3])
err_compare

```
Based on the training misclassification rate and CV misclassification rate for different `k_nn`
value, we would choose the model with `k_nn` equal to 1 for both of them since when `k_nn` is
1, the training misclassification rate and CV misclassification rate are the lowest. In practice,
I would choose the model with lowest CV misclassification rates. The purpose of cross-validation
is to divide up data into several equal amount of data, then to loop through each chunk of data
as train data and others as testing data. For example, in our demonstration, when `k_cv` = 5, we
are dividing up the data into 5 chunks and each time there will be one chunk of data as training
data and other 4 chunks of data as testing data. The CV misclassification rates we got from 
`my_knn_cv` function is the mean of five misclassification rates. This would be a better and more
pracitical way to avoid overfit since we are testing the data for several times. Therefore, I
would go with the model with lowest CV misclassification rate.


## A tutorial for `my_rf_cv`  

For this demonstration, we will use `my_rf_cv` function to predict `body_mass_g` using covariates
`bill_length_mm`, `bill_depth_mm`, and `flipper_length_mm.`  
We will iterate through k as 2, 5 and 10:  
  * For each value of k, run your function 30 times.
  * For each of the 30 iterations, store the CV estimated MSE.

```{r}
# store 30 CV estimated MSE when k = 2
mse_2 <- rep(NA, 30)
# store 30 CV estimated MSE when k = 2
mse_5 <- rep(NA, 30)
# store 30 CV estimated MSE when k = 2
mse_10 <- rep(NA, 30)
# For each k, run the function 30 times to generate 30 CV errors.
for (i in 1:30) {
  mse_2[i] <- my_rf_cv(2)
  mse_5[i] <- my_rf_cv(5)
  mse_10[i] <- my_rf_cv(10)
}

# mse table for k = 2
mse_2_df <- data.frame("k_value" = as.factor(rep(2, 30)), "cv_estimated_mse" = mse_2)
# mse table for k = 5
mse_5_df <- data.frame("k_value" = as.factor(rep(5, 30)), "cv_estimated_mse" = mse_5)
# mse table for k = 10
mse_10_df <- data.frame("k_value" = as.factor(rep(10, 30)), "cv_estimated_mse" = mse_10)

# combine all three dataframes
mse_df <- rbind(mse_2_df, mse_5_df, mse_10_df)

```

Now, we will make 3 boxplots to display these data in an informative way. Each boxplot is associated with each value of $k$, representing 30 simulations.

```{r fig2, fig.height = 3, fig.width = 5, fig.align = "center"}
# Make 3 boxplots.
ggplot(data = mse_df, aes(x = k_value, y = cv_estimated_mse, fill = k_value)) +
  geom_boxplot() +
  theme_bw(base_size = 15) +
  labs(title = "MSE for 30 randomForsts with number of fold of 2, 5 and 10", 
       x = "k-fold Cross-validation", 
       y = "CV Estimated MSE") +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
# calculate the mean and sd when k = 2
mse_2_mean <- mean(mse_2_df$cv_estimated_mse)
mse_2_sd <- sd(mse_2_df$cv_estimated_mse)

# calculate the mean and sd when k = 5
mse_5_mean <- mean(mse_5_df$cv_estimated_mse)
mse_5_sd <- sd(mse_5_df$cv_estimated_mse)

# calculate the mean and sd when k = 10
mse_10_mean <- mean(mse_10_df$cv_estimated_mse)
mse_10_sd <- sd(mse_10_df$cv_estimated_mse)

mse_table <- data.frame("k_value" = c(2, 5, 10), 
                        "MSE_mean" = c(mse_2_mean, mse_5_mean, mse_10_mean),
                        "MSE_sd" = c(mse_2_sd, mse_5_sd, mse_10_sd))
mse_table
```
From the boxplots as well as the mse table, we can tell that the MSE mean and MSE standard 
deviation when k = 10 is lower than when k = 2 and k = 5. We can tell directly from the boxplots
that as the value of k increase, the MSE mean and MSE sd decrease. The mse table shows that 
trends as well. This makes sense since when there are more folds, there will be higher accuracy,
which would lead to less mean squared error.
